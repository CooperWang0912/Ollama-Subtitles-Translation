{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title **Ollama AI文本翻译/Ollama AI Translation:**\n",
        "\n",
        "# @markdown **</br>**<font size=\"2\"> 使用Ollama来执行本地部署的LLM，想要免费使用AI翻译的用户可以执行以下单元格。\n",
        "# @markdown **</br>**阅读项目文档以了解更多。</font>\n",
        "# @markdown **</br>**<font size=\"2\"> Since Ollama provides the ability to deploy LLMs locally, the users that want to use free AI translations can execute the following blocks of code.\n",
        "# @markdown **</br>**Then generate bilingual subtitle files in same sub style.Read documentaion to learn more.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title **下载Ollama Library/Importing Ollama Library**\n",
        "\n",
        "!sudo apt update\n",
        "\n",
        "!sudo apt install -y pciutils\n",
        "\n",
        "!curl https://ollama.ai/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title **初始化Ollama Server/Starting Ollama Server**\n",
        "\n",
        "\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_serve():\n",
        "  subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "time.sleep(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title **下载Ollama LLM/Pulling Ollama LLM**\n",
        "\n",
        "# @markdown **用户可以在这里选择想要部署的LLM**\n",
        "\n",
        "# @markdown **The user can choose the LLM type for the deployment here**\n",
        "\n",
        "model_type = \"deepseek-r1:14b\"  # @param [\"llama3.2:3b\", \"deepseek-r1:7b\", \"deepseek-r1:14b\", \"deepseek-llm\"]\n",
        "\n",
        "! ollama pull $model_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title **进行翻译/Translating The Subtitles**\n",
        "\n",
        "! pip install -U langchain-ollama\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import codecs\n",
        "import regex as re\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output\n",
        "\n",
        "uploaded = files.upload()\n",
        "sub_name = list(uploaded.keys())[0]\n",
        "sub_basename = Path(sub_name).stem\n",
        "\n",
        "clear_output()\n",
        "\n",
        "! pip install pysubs2\n",
        "\n",
        "import pysubs2\n",
        "\n",
        "output_format = \"ass\"  # @param [\"ass\",\"srt\"]\n",
        "\n",
        "target_language = 'zh-hans'# @param [\"zh-hans\",\"english\"]\n",
        "template = \"You are a language expert.Your task is to translate the input subtitle text, sentence by sentence, into {target_language}.However, please utilize the context to improve the accuracy and quality of translation.Please be aware that the input text could contain typos and grammar mistakes, utilize the context to correct the translation.Please return only translated content and do not include the origin text. Do not return your thinking process and only return the translated text. Please do not use any punctuation around the returned text.Please do not translate people's name and leave it as original language. Here is the text to translate: {text}\\\"\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown <font size=\"4\">Default prompt: </br>\n",
        "# @markdown ```You are a language expert.```</br>\n",
        "# @markdown ```Your task is to translate the input subtitle text, sentence by sentence, into {target_language}.```</br>\n",
        "# @markdown ```Please utilize the context to improve the accuracy and quality of translation.```</br>\n",
        "# @markdown ```Please be aware that the input text could contain typos and grammar mistakes, utilize the context to correct the translation.```</br>\n",
        "# @markdown ```Please return only translated content and do not include the origin text.```</br>\n",
        "# @markdown ```Do not return your thinking process and only return the translated text.```</br>\n",
        "# @markdown ```Please do not use any punctuation around the returned text.```</br>\n",
        "# @markdown ```Please do not translate people's name and leave it as original language.```</br>\n",
        "# @markdown ```Here is the text to translate: {text}```</br>\n",
        "\n",
        "llm = OllamaLLM(model=model_type)\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "chain = prompt | llm\n",
        "\n",
        "def translate(prompt, language, text):\n",
        "        # print(text)\n",
        "        # self.rotate_key()\n",
        "        t_text = chain.invoke({\"target_language\": language, \"text\": text})\n",
        "        print(t_text)\n",
        "        if model_type == \"deepseek-r1:14b\" or model_type == \"deepseek-r1:7b\":\n",
        "          return t_text.split(\"\\n\")[len(t_text.split(\"\\n\"))-1]\n",
        "        else:\n",
        "          return t_text\n",
        "\n",
        "class SubtitleTranslator():\n",
        "    def __init__(self, sub_src):\n",
        "        self.sub_src = sub_src\n",
        "        self.translations = []\n",
        "\n",
        "    def translate_by_line(self):\n",
        "        sub_trans = pysubs2.load(self.sub_src)\n",
        "        total_lines = len(sub_trans)\n",
        "        for line in tqdm(sub_trans,total = total_lines):\n",
        "            line_trans = translate(prompt, target_language, line.text)\n",
        "            line.text += (line_trans)\n",
        "            print(line_trans)\n",
        "            self.translations.append(line_trans)\n",
        "        return sub_trans, self.translations\n",
        "\n",
        "clear_output()\n",
        "\n",
        "t = SubtitleTranslator(sub_src=sub_name)\n",
        "\n",
        "translation, _, = t.translate_by_line()\n",
        "\n",
        "if output_format == 'ass':\n",
        "  translation.save(sub_basename + '_translation.ass')\n",
        "  files.download(sub_basename + '_translation.ass')\n",
        "elif output_format == 'srt':\n",
        "  translation.save(sub_basename + '_translation.srt')\n",
        "  files.download(sub_basename + '_translation.srt')\n",
        "\n",
        "print('双语字幕生成完毕 All done!')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
